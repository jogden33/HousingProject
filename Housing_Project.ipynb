{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Housing Project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIlXhvcGp2wFjNQSjvhABE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jogden33/HousingProject/blob/master/Housing_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pBpkljTPrmu"
      },
      "source": [
        "Import Pandas and do some data exploration on the three datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0vlsUlnG43b"
      },
      "source": [
        "import pandas as pd\n",
        "import re as re\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Read in our 3 datasets\n",
        "Housing = pd.read_csv(\"/housing-info.csv\")\n",
        "Income = pd.read_csv(\"/income-info.csv\")\n",
        "County = pd.read_csv(\"/zip-city-county-state.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnDN9hYUHB0p"
      },
      "source": [
        "# Find every instance of corrupt data and replace it with NaN\n",
        "Housing.replace(to_replace = r'^[A-Z]{4}$', value=np.nan, regex=True, inplace=True)\n",
        "\n",
        "# Delete the rows where guid is NaN\n",
        "Housing.dropna(axis = 0, subset = ['guid'], inplace = True)\n",
        "Housing.head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "535wJfyfRN5W"
      },
      "source": [
        "# Replace missing values for several of the columns\n",
        "Housing.loc[Housing['housing_median_age'].isnull(), 'housing_median_age'] = Housing['housing_median_age'].apply(lambda v: np.random.randint(10,50))\n",
        "Housing.loc[Housing['total_rooms'].isnull(), 'total_rooms'] = Housing['total_rooms'].apply(lambda v: np.random.randint(1000, 2000))\n",
        "Housing.loc[Housing['total_bedrooms'].isnull(), 'total_bedrooms'] = Housing['total_bedrooms'].apply(lambda v: np.random.randint(1000,2000))\n",
        "Housing.loc[Housing['population'].isnull(), 'population'] = Housing['population'].apply(lambda v: np.random.randint(5000, 10000))\n",
        "Housing.loc[Housing['households'].isnull(), 'households'] = Housing['households'].apply(lambda v: np.random.randint(500, 2500))\n",
        "Housing.loc[Housing['median_house_value'].isnull(), 'median_house_value'] = Housing['median_house_value'].apply(lambda v: np.random.randint(100000, 250000))\n",
        "Housing.head(25)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q6ldcuYcqZO"
      },
      "source": [
        "# Create a dataset that strips down non-missing values for zip, city, state, and country. This gives us a reference table to allow for imputation of missing\n",
        "# ZIP codes in our mail file.\n",
        "CountyImputed = County[['zip_code', 'city', 'state', 'county']].copy()\n",
        "CountyImputed.replace(to_replace = r'^[A-Z]{4}$', value=np.nan, regex=True, inplace=True)\n",
        "CountyImputed.dropna(axis = 0, subset = ['zip_code'], inplace = True)\n",
        "CountyImputed.sort_values(by=['state','city','county'], inplace = True)\n",
        "CountyImputed['NewZip']=CountyImputed.zip_code.str[:1]+'0000'\n",
        "CountyImputed.drop_duplicates(['state','city','county'], keep='last', inplace = True)\n",
        "CountyImputed.tail(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L57uisf3l0Js"
      },
      "source": [
        "Income.head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H49-fLbQmdvL"
      },
      "source": [
        "# Clean the income file \n",
        "Income.replace(to_replace = r'^[A-Z]{4}$', value=np.nan, regex=True, inplace=True)\n",
        "Income.dropna(axis = 0, subset = ['guid'], inplace = True)\n",
        "Income.loc[Income['median_income'].isnull(), 'median_income'] = Income['median_income'].apply(lambda v: np.random.randint(100000, 750000))\n",
        "Income.head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5pRtSe8pLg3"
      },
      "source": [
        "#County.replace(to_replace = r'^[A-Z]{4}$', value=np.nan, regex=True, inplace=True)\n",
        "#County.dropna(axis = 0, subset = ['guid'], inplace = True)\n",
        "#County.head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4M6PphMtVen"
      },
      "source": [
        "# Begin merging our datasets\n",
        "Housing_Zip = Housing.merge(County, how = 'outer', on = ['guid', 'zip_code'])\n",
        "Housing_Zip.head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLe_gY1Lx256"
      },
      "source": [
        "Housing_Zip_Income = Housing_Zip.merge(Income, how = 'outer', on = ['guid', 'zip_code'])\n",
        "Housing_Zip_Income.head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXzi8oIgtF3T"
      },
      "source": [
        "# Step 4: Create a reference dataset with non-duplicate zip codes so we can impute missing Zip Codes\n",
        "ZipTable = County[['zip_code', 'city', 'state', 'county']].copy()\n",
        "\n",
        "# Next, replace all of the corrupted zip codes with NaN (missing value) and drop all of those rows\n",
        "ZipTable.replace(to_replace = r'^[A-Z]{4}$', value=np.nan, regex=True, inplace=True)\n",
        "ZipTable.dropna(axis = 0, subset = ['zip_code'], inplace = True)\n",
        "\n",
        "# Finally, sort the data by state, city, county, in that order and create a new variable that strings the first\n",
        "# number from the zip code to '0000'. Also, get rid of any duplicates.\n",
        "ZipTable.sort_values(by=['state','city','county'], inplace = True)\n",
        "ZipTable['NewZip']=ZipTable.zip_code.str[:1]+'0000'\n",
        "ZipTable.drop_duplicates(['state','city','county'], keep='last', inplace = True)\n",
        "\n",
        "# Now we have a clean reference dataset from which we can join our other two datasets to obtain zip codes for missing values\n",
        "Housing_Zip_Income2 = Housing_Zip_Income.merge(ZipTable, how = 'left', on = ['state', 'city', 'county'])\n",
        "Housing_Zip_Income2.tail(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GemRzXqx_Yj"
      },
      "source": [
        "Housing_Zip_Income2.loc[Housing_Zip_Income2['zip_code_x'].isnull(), 'zip_code_x'] = Housing_Zip_Income2[\"NewZip\"]\n",
        "Housing_Zip_Income2['id'] = range(1,1+len(Housing_Zip_Income2))\n",
        "Housing_Zip_Income2.rename(columns = {'zip_code_x':'zip_code', 'housing_median_age':'median_age'}, inplace = True)\n",
        "HousingData = Housing_Zip_Income2[['id','guid','zip_code', 'city','state','county','median_age','total_rooms','total_bedrooms','population','households','median_income','median_house_value']].copy()\n",
        "HousingData.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}